{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khoubaib30/dbmsproject/blob/main/Etlprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7lZg1Z_dGSA",
        "outputId": "f300d789-9d95-46f4-a6a0-b1b4e67dac0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-bo4L65nox_"
      },
      "source": [
        "convertion of presence sheet to json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppiSI2w2npOn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load Presence data\n",
        "presence_data = pd.read_excel('/content/drive/MyDrive/presence.xlsx')  # Assuming your Excel file is named 'presence.xlsx'\n",
        "\n",
        "# Convert to JSON and save\n",
        "presence_data.to_json('presencejson.json', orient='records', date_format='iso')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hwi7E1uGzmEX"
      },
      "source": [
        "conversion of absence sheet to json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBXuBD-gzqgr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load Presence data\n",
        "presence_data = pd.read_excel('/content/drive/MyDrive/absence.xlsx')  # Assuming your Excel file is named 'presence.xlsx'\n",
        "\n",
        "# Convert to JSON and save\n",
        "presence_data.to_json('absencejson.json', orient='records', date_format='iso')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pQVUd0C1Dsq"
      },
      "source": [
        "conversion of congee sheet to json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bH8sbNxS1LWU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load Presence data\n",
        "presence_data = pd.read_excel('/content/drive/MyDrive/congees.xlsx')  # Assuming your Excel file is named 'presence.xlsx'\n",
        "\n",
        "# Convert to JSON and save\n",
        "presence_data.to_json('congeejson.json', orient='records', date_format='iso')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MPxPlwZUef8"
      },
      "source": [
        "unify the names in all databases zeyda ()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yN7W-HuUhjG"
      },
      "outputs": [],
      "source": [
        "!pip install rapidfuzz\n",
        "\n",
        "import pandas as pd\n",
        "from rapidfuzz import process\n",
        "\n",
        "# Load JSON files into pandas DataFrames\n",
        "employees_df = pd.read_json('/content/drive/MyDrive/employeesjson.json')\n",
        "absence_df = pd.read_json('/content/drive/MyDrive/absencejson.json')\n",
        "presence_df = pd.read_json('/content/drive/MyDrive/presencejson.json')\n",
        "congee_df = pd.read_json('/content/drive/MyDrive/congeejson.json')\n",
        "\n",
        "# Preprocess the data (lowercase, remove special characters)\n",
        "employees_df['preprocessed_name'] = employees_df['prenom'].str.lower().str.replace('[^a-zA-Z0-9]', '')\n",
        "\n",
        "absence_df['preprocessed_name'] = absence_df['prenom'].str.lower().str.replace('[^a-zA-Z0-9]', '')\n",
        "presence_df['preprocessed_name'] = presence_df['prenom'].str.lower().str.replace('[^a-zA-Z0-9]', '')\n",
        "congee_df['preprocessed_name'] = congee_df['prenom'].str.lower().str.replace('[^a-zA-Z0-9]', '')\n",
        "\n",
        "# Function to find the closest match using rapidfuzz\n",
        "def find_closest_match(name, reference_names):\n",
        "    result = process.extractOne(name, reference_names)\n",
        "    return result\n",
        "\n",
        "# Initialize lists to store unmatched names and matches for inspection\n",
        "unmatched_names = []\n",
        "matches = []\n",
        "\n",
        "# Map names and collect unmatched names and matches\n",
        "for df in [absence_df, presence_df, congee_df]:\n",
        "    df['match_result'] = df['preprocessed_name'].apply(lambda x: find_closest_match(x, employees_df['preprocessed_name']))\n",
        "    df['mapped_name'] = df['match_result'].apply(lambda x: x[0] if x[1] >= 70 else None)  # Adjust threshold as needed\n",
        "    unmatched_names.extend(df[df['mapped_name'].isnull()]['prenom'].tolist())\n",
        "    matches.extend(df[df['mapped_name'].notnull()][['prenom', 'mapped_name', 'match_result']].values.tolist())\n",
        "\n",
        "# Print unmatched names\n",
        "print(\"Unmatched Names:\")\n",
        "print(unmatched_names)\n",
        "\n",
        "# Print matches for inspection\n",
        "print(\"\\nMatches for Inspection:\")\n",
        "for match in matches:\n",
        "    print(f\"Original: {match[0]}, Mapped: {match[1]}, Score: {match[2][1]}\")\n",
        "\n",
        "# Update names in absence_df, presence_df, and congee_df using mapped names\n",
        "for df in [absence_df, presence_df, congee_df]:\n",
        "    df['prenom'] = df['mapped_name'].apply(lambda x: employees_df.loc[employees_df['preprocessed_name'] == x, 'prenom'].iloc[0] if x else None)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "for df in [absence_df, presence_df, congee_df]:\n",
        "    df.drop(['preprocessed_name', 'mapped_name', 'match_result'], axis=1, inplace=True)\n",
        "\n",
        "# Save the updated DataFrames to new JSON files without changing date and time formats\n",
        "absence_df.to_json('absenceunified.json', orient='records')\n",
        "presence_df.to_json('presenceunified.json', orient='records')\n",
        "congee_df.to_json('congeeunified.json', orient='records')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "standardize the date and time fields, you can convert all date fields to the format YYYY-MM-DD and time fields to the format HH:MM:SS."
      ],
      "metadata": {
        "id": "2SDS5uKVT3w3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary library\n",
        "import pandas as pd\n",
        "\n",
        "# Function to standardize date format to YYYY-MM-DD\n",
        "def standardize_date_format(date_series):\n",
        "    return pd.to_datetime(date_series).dt.strftime('%Y-%m-%d')\n",
        "\n",
        "# Function to standardize time format to HH:MM:SS (with seconds as \"00\")\n",
        "def standardize_time_format(time_series):\n",
        "    return pd.to_datetime(time_series, format='%H:%M', errors='coerce').dt.strftime('%H:%M:%S')\n",
        "\n",
        "# Load the JSON files into pandas DataFrames\n",
        "absence_df = pd.read_json('/content/drive/MyDrive/absenceunified.json')\n",
        "congee_df = pd.read_json('/content/drive/MyDrive/congeeunified.json')\n",
        "presence_df = pd.read_json('/content/drive/MyDrive/presenceunified.json')\n",
        "\n",
        "# Standardizing date fields to YYYY-MM-DD format\n",
        "absence_df['Date'] = standardize_date_format(absence_df['Date'])\n",
        "congee_df['DATE DEBUT'] = standardize_date_format(congee_df['DATE DEBUT'])\n",
        "congee_df['REPRISE'] = standardize_date_format(congee_df['REPRISE'])\n",
        "presence_df['Date'] = standardize_date_format(presence_df['Date'])\n",
        "\n",
        "# Define the time fields for each DataFrame\n",
        "absence_time_fields = ['Enregistrement', 'Depart', 'Pointer entree', 'Pointer sortie']\n",
        "presence_time_fields = ['Premier pointage', 'Dernier pointage']\n",
        "\n",
        "# Standardizing time fields in absence_df and presence_df\n",
        "for field in absence_time_fields:\n",
        "    if field in absence_df.columns:\n",
        "        absence_df[field] = standardize_time_format(absence_df[field])\n",
        "\n",
        "for field in presence_time_fields:\n",
        "    if field in presence_df.columns:\n",
        "        presence_df[field] = standardize_time_format(presence_df[field])\n",
        "\n",
        "absence_df.to_json('absencedate.json', orient='records', date_format='iso')\n",
        "congee_df.to_json('congeedate.json', orient='records', date_format='iso')\n",
        "presence_df.to_json('presencedate.json', orient='records', date_format='iso')"
      ],
      "metadata": {
        "id": "YT7bTCzoT4IV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYoL1x0ZIphQ"
      },
      "source": [
        "assign a unique id to each departement for better analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTCl-K8duZXJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Load data from the \"employees\" JSON file into a Pandas DataFrame\n",
        "df_employees = pd.read_json('/content/drive/MyDrive/employeesjson.json')\n",
        "\n",
        "# Extract unique departments and assign unique IDs\n",
        "unique_departments = df_employees['departement'].unique()\n",
        "department_id_mapping = {department: idx + 1 for idx, department in enumerate(unique_departments)}\n",
        "\n",
        "# Add department IDs to the original file\n",
        "for index, row in df_employees.iterrows():\n",
        "    df_employees.at[index, 'department_id'] = department_id_mapping[row['departement']]\n",
        "\n",
        "# Save the updated data back to the original file\n",
        "df_employees.to_json('/content/drive/MyDrive/employeesfinal.json', orient='records')\n",
        "\n",
        "# Create a new JSON file with department IDs\n",
        "department_id_json = [{'department_id': department_id, 'department': department} for department, department_id in department_id_mapping.items()]\n",
        "with open('/content/drive/MyDrive/department_id.json', 'w') as json_file:\n",
        "    json.dump(department_id_json, json_file, indent=2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "count of emplyees per department"
      ],
      "metadata": {
        "id": "KQ9sJBZCua18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Load the department_id JSON file\n",
        "with open('/content/drive/MyDrive/department_id.json', 'r') as file:\n",
        "    department_id_data = json.load(file)\n",
        "\n",
        "# Load the employeesfinal JSON file\n",
        "df_employees = pd.read_json('/content/drive/MyDrive/employeesfinal.json')\n",
        "\n",
        "# Create a mapping from department names to department IDs\n",
        "department_name_to_id = {item['department']: item['department_id'] for item in department_id_data}\n",
        "\n",
        "# Count the number of employees per department name\n",
        "department_name_count = df_employees['departement'].value_counts().to_dict()\n",
        "\n",
        "# Update the department_id_data with the number of employees per department ID\n",
        "for item in department_id_data:\n",
        "    department_name = item['department']\n",
        "    department_id = item['department_id']\n",
        "    item['number_of_employees'] = department_name_count.get(department_name, 0)\n",
        "\n",
        "# Save the updated department_id_data back to JSON\n",
        "with open('updated_department_id.json', 'w') as file:\n",
        "    json.dump(department_id_data, file, indent=2)"
      ],
      "metadata": {
        "id": "oJZtFgemtEYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6dANP6ZkiEg"
      },
      "source": [
        "include the ID,department and department_id in congee file\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load the data from the files\n",
        "with open('/content/drive/MyDrive/congeedate.json', 'r') as file:\n",
        "    finalcongee_data = json.load(file)\n",
        "\n",
        "with open('/content/drive/MyDrive/employeesfinal.json', 'r') as file:\n",
        "    employees_data = json.load(file)\n",
        "\n",
        "# Create a mapping of 'prenom' to 'id', 'departement', and 'department_id' from the employees data\n",
        "employee_dept_mapping = {employee['prenom']: {'id': employee['ID'], 'departement': employee['departement'], 'department_id': employee['department_id']}\n",
        "                         for employee in employees_data}\n",
        "\n",
        "# Update the finalcongee data with the department information and employee ID\n",
        "for record in finalcongee_data:\n",
        "    prenom = record['prenom']\n",
        "    if prenom in employee_dept_mapping:\n",
        "        record.update(employee_dept_mapping[prenom])\n",
        "\n",
        "# Save the updated finalcongee data to a new JSON file\n",
        "with open('updated_congee.json', 'w') as file:\n",
        "    json.dump(finalcongee_data, file)"
      ],
      "metadata": {
        "id": "UduOaqO8Izam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9aVivC6m1tp"
      },
      "source": [
        "include department_id in presence and absence files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNcTuIXBnDic"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "def load_json_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        return pd.DataFrame(json.load(file))\n",
        "\n",
        "# Load the JSON files\n",
        "department_id_df = load_json_file('/content/drive/MyDrive/department_id.json')\n",
        "absence_df = load_json_file('/content/drive/MyDrive/absencedate.json')\n",
        "presence_df = load_json_file('/content/drive/MyDrive/presencedate.json')\n",
        "\n",
        "# Merge absence_df with department_id_df to get 'department_id' based on 'departement'\n",
        "absence_df = pd.merge(absence_df, department_id_df, left_on='departement', right_on='department', how='left')\n",
        "\n",
        "# Merge presence_df with department_id_df to get 'department_id' based on 'departement'\n",
        "presence_df = pd.merge(presence_df, department_id_df, left_on='departement', right_on='department', how='left')\n",
        "\n",
        "# Drop redundant 'department' column after the merge\n",
        "absence_df.drop('department', axis=1, inplace=True)\n",
        "presence_df.drop('department', axis=1, inplace=True)\n",
        "\n",
        "# Save the updated DataFrames to new JSON files without altering date and time formats\n",
        "\n",
        "absence_df.to_json('absence_updated.json', orient='records', date_format='iso')\n",
        "presence_df.to_json('presence_updated.json', orient='records', date_format='iso')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbNWU4lZk-uf"
      },
      "source": [
        "update of \"Premier pointage\" and \"dernier pointage\" fields and cration of columns of total working time and supplemantay working time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvF6Tfsu1gPW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Load the JSON file\n",
        "df = pd.read_json('/content/drive/MyDrive/presence_updated.json')\n",
        "\n",
        "# Function to convert time to datetime.time object\n",
        "def convert_to_time(time_str):\n",
        "    if pd.isna(time_str):\n",
        "        return None\n",
        "    return datetime.strptime(time_str, '%H:%M:%S').time()\n",
        "\n",
        "# Function to calculate working hours\n",
        "def calculate_working_hours(start, end):\n",
        "    if start is None or end is None:\n",
        "        return 0\n",
        "    start_dt = datetime.combine(datetime.today(), start)\n",
        "    end_dt = datetime.combine(datetime.today(), end)\n",
        "    if start_dt > end_dt:\n",
        "        end_dt += timedelta(days=1)\n",
        "    return round((end_dt - start_dt).seconds / 3600, 1)\n",
        "\n",
        "# Updating \"Premier pointage\" and \"Dernier pointage\" columns\n",
        "df['Premier pointage'] = df['Premier pointage'].apply(convert_to_time)\n",
        "df['Dernier pointage'] = df['Dernier pointage'].apply(convert_to_time)\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    if row['Jour de semaine'] not in ['Samedi', 'Dimanche']:\n",
        "        if row['Premier pointage'] is not None and row['Dernier pointage'] is not None:\n",
        "            if row['Premier pointage'] == row['Dernier pointage']:\n",
        "                if row['Premier pointage'] < datetime.strptime('12:00:00', '%H:%M:%S').time():\n",
        "                    df.at[index, 'Dernier pointage'] = datetime.strptime('17:30:00', '%H:%M:%S').time()\n",
        "                elif row['Premier pointage'] > datetime.strptime('12:00:00', '%H:%M:%S').time():\n",
        "                    df.at[index, 'Premier pointage'] = datetime.strptime('08:30:00', '%H:%M:%S').time()\n",
        "\n",
        "# Calculating total working time\n",
        "df['Total working time'] = df.apply(lambda row: calculate_working_hours(row['Premier pointage'], row['Dernier pointage']), axis=1)\n",
        "\n",
        "# Calculating supplementary working time\n",
        "df['Supplementary working time'] = df.apply(lambda row: max(0, row['Total working time'] - 9) if row['Jour de semaine'] not in ['Samedi', 'Dimanche'] else 0, axis=1)\n",
        "\n",
        "# Converting Date column to string in yyyy-mm-dd format\n",
        "df['Date'] = pd.to_datetime(df['Date']).dt.strftime('%Y-%m-%d')\n",
        "\n",
        "# Saving the updated presence file\n",
        "df.to_json('final_presence.json', orient='records', date_format='iso')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBcU4D8x33Hx"
      },
      "source": [
        "create a field totale congee in the congee.json file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbFn4Bt134JN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load Absence data from JSON\n",
        "absence_data = pd.read_json('/content/drive/MyDrive/updated_congee.json', orient='records')\n",
        "\n",
        "# Convert 'REPRISE' and 'DATE DEBUT' to datetime\n",
        "absence_data['REPRISE'] = pd.to_datetime(absence_data['REPRISE'], errors='coerce')\n",
        "absence_data['DATE DEBUT'] = pd.to_datetime(absence_data['DATE DEBUT'])\n",
        "\n",
        "# Calculate 'Total Congee' column\n",
        "absence_data['Total Congee'] = (absence_data['REPRISE'] - absence_data['DATE DEBUT']).dt.total_seconds() / (24 * 3600)\n",
        "\n",
        "# Adjust 'Total Congee' for cases where 'REPRISE' is empty or 'REMARQUE' contains \"1/2 JR\"\n",
        "absence_data.loc[absence_data['REPRISE'].isnull() | (absence_data['REMARQUE'] == \"1/2 JR\"), 'Total Congee'] = 0.5\n",
        "\n",
        "# Format the date columns as 'yyyy-mm-dd' strings\n",
        "absence_data['REPRISE'] = absence_data['REPRISE'].dt.strftime('%Y-%m-%d')\n",
        "absence_data['DATE DEBUT'] = absence_data['DATE DEBUT'].dt.strftime('%Y-%m-%d')\n",
        "\n",
        "# Print the updated Absence data for debugging\n",
        "print(\"\\nUpdated Absence Data:\")\n",
        "print(absence_data[['REPRISE', 'DATE DEBUT', 'REMARQUE', 'Total Congee']])\n",
        "\n",
        "# Save the updated Absence data to a new JSON file with 'yyyy-mm-dd' format\n",
        "absence_data.to_json('finalcongee.json', orient='records', date_format='iso', default_handler=str)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "create the fact table"
      ],
      "metadata": {
        "id": "tAzCHiOV3-j1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the data from each file\n",
        "presence_data = pd.read_json('/content/drive/MyDrive/final_presence.json')\n",
        "absence_data = pd.read_json('/content/drive/MyDrive/finalabsence.json')\n",
        "congee_data = pd.read_json('/content/drive/MyDrive/finalcongee.json')\n",
        "\n",
        "# Process the presence data\n",
        "presence_data['Weekend'] = presence_data['Jour de semaine'].isin(['Samedi', 'Dimanche']).astype(int)\n",
        "presence_data['Present'] = 1 - presence_data['Weekend']\n",
        "presence_data['Congee'] = 0\n",
        "presence_data['Absent'] = 0\n",
        "presence_data['Total_Working_Time'] = presence_data['Total working time']\n",
        "presence_data['Supplementary_Working_Time'] = presence_data['Supplementary working time']\n",
        "presence_data_processed = presence_data[['department_id', 'ID', 'Date', 'Total_Working_Time', 'Supplementary_Working_Time', 'Absent', 'Present', 'Weekend', 'Congee']]\n",
        "presence_data_processed = presence_data_processed.rename(columns={'department_id': 'Department_id', 'ID': 'Employee_id'})\n",
        "\n",
        "# Process the absence data\n",
        "absence_data['Absent'] = 1\n",
        "absence_data['Present'] = 0\n",
        "absence_data['Weekend'] = 0\n",
        "absence_data['Congee'] = 0\n",
        "absence_data['Total_Working_Time'] = 0\n",
        "absence_data['Supplementary_Working_Time'] = 0\n",
        "absence_data_processed = absence_data[['department_id', 'ID', 'Date', 'Total_Working_Time', 'Supplementary_Working_Time', 'Absent', 'Present', 'Weekend', 'Congee']]\n",
        "absence_data_processed = absence_data_processed.rename(columns={'department_id': 'Department_id', 'ID': 'Employee_id'})\n",
        "\n",
        "# Process the congee data\n",
        "congee_data_processed = pd.DataFrame()\n",
        "for index, row in congee_data.iterrows():\n",
        "    if row['Total Congee'] == 0.5:\n",
        "        # Create a single-day entry with congee 0.5\n",
        "        congee_entry = pd.DataFrame({\n",
        "            'Department_id': [row['department_id']],\n",
        "            'Employee_id': [row['id']],\n",
        "            'Date': [row['DATE DEBUT']],\n",
        "            'Total_Working_Time': [0],\n",
        "            'Supplementary_Working_Time': [0],\n",
        "            'Absent': [0],\n",
        "            'Present': [0],\n",
        "            'Weekend': [0],\n",
        "            'Congee': [0.5]\n",
        "        })\n",
        "    else:\n",
        "        # Create entries for each date between 'DATE DEBUT' and 'REPRISE'\n",
        "        date_range = pd.date_range(start=row['DATE DEBUT'], end=row['REPRISE'], closed='left')\n",
        "        congee_entry = pd.DataFrame({\n",
        "            'Department_id': row['department_id'],\n",
        "            'Employee_id': row['id'],\n",
        "            'Date': date_range,\n",
        "            'Total_Working_Time': 0,\n",
        "            'Supplementary_Working_Time': 0,\n",
        "            'Absent': 0,\n",
        "            'Present': 0,\n",
        "            'Weekend': 0,\n",
        "            'Congee': 1\n",
        "        })\n",
        "    congee_data_processed = pd.concat([congee_data_processed, congee_entry], ignore_index=True)\n",
        "\n",
        "# Combine all processed data\n",
        "combined_data = pd.concat([presence_data_processed, absence_data_processed, congee_data_processed], ignore_index=True)\n",
        "\n",
        "# Drop rows with missing values in Department_id or Employee_id\n",
        "combined_data_cleaned = combined_data.dropna(subset=['Department_id', 'Employee_id'])\n",
        "\n",
        "# Convert Department_id and Employee_id to integer type\n",
        "combined_data_cleaned['Department_id'] = combined_data_cleaned['Department_id'].astype(int)\n",
        "combined_data_cleaned['Employee_id'] = combined_data_cleaned['Employee_id'].astype(int)\n",
        "\n",
        "# Round float columns to the first decimal\n",
        "combined_data_cleaned['Total_Working_Time'] = combined_data_cleaned['Total_Working_Time'].round(1)\n",
        "combined_data_cleaned['Supplementary_Working_Time'] = combined_data_cleaned['Supplementary_Working_Time'].round(1)\n",
        "combined_data_cleaned['Congee'] = combined_data_cleaned['Congee'].round(1)\n",
        "\n",
        "# Ensure that the 'Date' column is in datetime format and sort by Date in ascending order\n",
        "combined_data_sorted = combined_data_cleaned.copy()\n",
        "combined_data_sorted['Date'] = pd.to_datetime(combined_data_sorted['Date'])\n",
        "combined_data_sorted = combined_data_sorted.sort_values(by='Date')\n",
        "# Save the final output to a CSV file\n",
        "combined_data_sorted.to_csv('/content/drive/MyDrive/fact1.csv', index=False)\n"
      ],
      "metadata": {
        "id": "04ShHTT_3-2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "create dimensions"
      ],
      "metadata": {
        "id": "pru4o2RQ6wHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Load the data from the files\n",
        "# Replace these lines with your actual file loading code\n",
        "with open('/content/drive/MyDrive/final_presence.json', 'r') as file:\n",
        "    presence_data = json.load(file)\n",
        "\n",
        "with open('/content/drive/MyDrive/finalabsence.json', 'r') as file:\n",
        "    absence_data = json.load(file)\n",
        "\n",
        "with open('/content/drive/MyDrive/finalcongee.json', 'r') as file:\n",
        "    congee_data = json.load(file)\n",
        "\n",
        "with open('/content/drive/MyDrive/employeesfinal.json', 'r') as file:\n",
        "    employee_data = json.load(file)\n",
        "\n",
        "with open('/content/drive/MyDrive/updated_department_id.json', 'r') as file:\n",
        "    department_data = json.load(file)\n",
        "\n",
        "# Convert the data to pandas DataFrames\n",
        "df_presence = pd.DataFrame(presence_data)\n",
        "df_absence = pd.DataFrame(absence_data)\n",
        "df_congee = pd.DataFrame(congee_data)\n",
        "df_employee = pd.DataFrame(employee_data)\n",
        "df_department = pd.DataFrame(department_data)\n",
        "\n",
        "\n",
        "# Convert 'Date' to datetime in presence, absence, and congee data\n",
        "df_presence['Date'] = pd.to_datetime(df_presence['Date'])\n",
        "df_absence['Date'] = pd.to_datetime(df_absence['Date'])\n",
        "df_congee['DATE DEBUT'] = pd.to_datetime(df_congee['DATE DEBUT'])\n",
        "df_congee['REPRISE'] = pd.to_datetime(df_congee['REPRISE'])\n",
        "\n",
        "# Create the Date dimension\n",
        "date_dim = pd.DataFrame(df_presence['Date'].unique(), columns=['Date'])\n",
        "date_dim['Date_id'] = date_dim.index + 1  # Assuming Date_id starts from 1\n",
        "date_dim['Year'] = date_dim['Date'].dt.year\n",
        "date_dim['Month'] = date_dim['Date'].dt.month\n",
        "date_dim['Day'] = date_dim['Date'].dt.day\n",
        "date_dim['Date'] = date_dim['Date'].dt.strftime('%Y-%m-%d')  # Format date as 'yyyy-mm-dd'\n",
        "\n",
        "# Create the Employee dimension with Department_id\n",
        "employee_dim = df_employee.rename(columns={'id': 'Employee_id', 'prenom': 'Name', 'departement': 'Department', 'department_id': 'Department_id'})\n",
        "# Drop the department name column from the employee dimension DataFrame\n",
        "employee_dim.drop(columns=['Department'], inplace=True)\n",
        "employee_dim['Department_id'] = employee_dim['Department_id'].astype(int)\n",
        "\n",
        "# Create the department dimension\n",
        "department_dim = df_department.rename(columns={'department': 'Name', 'department_id': 'Department_id', 'employee_count': 'Number_of_employees'})\n",
        "department_dim['Department_id'] = department_dim['Department_id'].astype(int)\n",
        "\n",
        "date_dim.to_csv('date_dimension.csv', index=False)\n",
        "employee_dim.to_csv('employee_dimension.csv', index=False)\n",
        "department_dim.to_csv('department_dimension.csv', index=False)"
      ],
      "metadata": {
        "id": "pl2JesPV6wtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "include Date_id in the fact table"
      ],
      "metadata": {
        "id": "-JHPmerbUTlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the combined employee data and the date dimension data\n",
        "combined_employee_data = pd.read_csv('/content/drive/MyDrive/fact1.csv')\n",
        "date_dimension_data = pd.read_csv('/content/drive/MyDrive/date_dimension.csv')\n",
        "\n",
        "# Convert 'Date' columns to datetime format for merging\n",
        "combined_employee_data['Date'] = pd.to_datetime(combined_employee_data['Date'])\n",
        "date_dimension_data['Date'] = pd.to_datetime(date_dimension_data['Date'])\n",
        "\n",
        "# Merge the combined employee data with the date dimension data to include the Date_id column\n",
        "combined_data_with_date_id = combined_employee_data.merge(date_dimension_data[['Date', 'Date_id']], on='Date', how='left')\n",
        "\n",
        "# Drop the 'Date' column from the merged data\n",
        "combined_data_with_date_id.drop('Date', axis=1, inplace=True)\n",
        "\n",
        "# Save the updated combined data to a new CSV file\n",
        "combined_data_with_date_id.to_csv('/content/drive/MyDrive/hrfacttable.csv', index=False)"
      ],
      "metadata": {
        "id": "_WJyHeIAUT7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "create record_id column"
      ],
      "metadata": {
        "id": "itboCaSc_Cyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the combined employee data and the date dimension data\n",
        "facttab = pd.read_csv('/content/drive/MyDrive/fact2.csv')\n",
        "\n",
        "# Add a 'record_id' column to the data, starting at 1 and incrementing for each row\n",
        "facttab['record_id'] = range(1, len(facttab) + 1)\n",
        "\n",
        "# Save the data with 'record_id' column to a new CSV file\n",
        "facttab.to_csv('hr_fact_table.csv', index=False)\n"
      ],
      "metadata": {
        "id": "37Ln9LPv_DTZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObD8e1L/fRYSgVVDLhy4u/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}